{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe98176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'text.txt'}, page_content='LangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.[3][4]\\n\\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.[5][6]\\n\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.[7]\\n\\nIn February 2024 LangChain released LangSmith, a closed-source observability and evaluation platform for LLM applications, and announced a US $25 million Series A led by Sequoia Capital.[8] On 14 May 2025 the company launched LangGraph Platform into general availability, providing managed infrastructure for deploying long-running, stateful AI agents.[9]\\n\\nIn April 2025, LangChain was featured in the Forbes AI 50 list.[10]\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "#TEXT\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(file_path=\"text.txt\")\n",
    "docu=loader.load()\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cf64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='AWS Certified AI Practitioner (AIF-C01) Exam Guide \\nIntroduction \\nThe AWS Certified AI Practitioner (AIF-C01) exam is intended for individuals who can \\neffectively demonstrate overall knowledge of AI/ML, generative AI technologies, and \\nassociated AWS services and tools, independent of a specific job role. \\nThe exam also validates a candidate’s ability to complete the following tasks: \\n• Understand AI, ML, and generative AI concepts, methods, and strategies in \\ngeneral and on AWS. \\n• Understand the appropriate use of AI/ML and generative AI technologies to ask \\nrelevant questions within the candidate’s organization. \\n• Determine the correct types of AI/ML technologies to apply to specific use \\ncases. \\n• Use AI, ML, and generative AI technologies responsibly. \\nTarget candidate description \\nThe target candidate should have up to 6 months of exposure to AI/ML technologies \\non AWS. The target candidate uses but does not necessarily build AI/ML solutions on \\nAWS. \\nRecommended AWS knowledge \\nThe target candidate should have the following AWS knowledge: \\n• Familiarity with the core AWS services (for example, Amazon EC2, Amazon S3, \\nAWS Lambda, and Amazon SageMaker) and AWS core services use cases \\n• Familiarity with the AWS shared responsibility model for security and \\ncompliance in the AWS Cloud \\n• Familiarity with AWS Identity and Access Management (IAM) for securing and \\ncontrolling access to AWS resources \\n• Familiarity with the AWS global infrastructure, including the concepts of AWS \\nRegions, Availability Zones, and edge locations \\n• Familiarity with AWS service pricing models \\nVersion 1.4 AIF-C01 1 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='Job tasks that are out of scope for the target candidate \\nThe following list contains job tasks that the target candidate is not expected to be \\nable to perform. This list is non-exhaustive. These tasks are out of scope for the exam: \\n• Developing or coding AI/ML models or algorithms \\n• Implementing data engineering or feature engineering techniques \\n• Performing hyperparameter tuning or model optimization \\n• Building and deploying AI/ML pipelines or infrastructure \\n• Conducting mathematical or statistical analysis of AI/ML models \\n• Implementing security or compliance protocols for AI/ML systems \\n• Developing and implementing governance frameworks and policies for AI/ML \\nsolutions \\nRefer to the Appendix for a list of in-scope AWS services and features and a list of \\nout-of-scope AWS services and features. \\nExam content \\nQuestion types \\nThe exam contains one or more of the following question types: \\n• Multiple choice: Has one correct response and three incorrect responses \\n(distractors). \\n• Multiple response: Has two or more correct responses out of five or more \\nresponse options. You must select all the correct responses to receive credit for \\nthe question. \\n• Ordering: Has a list of 3–5 responses to complete a specified task. You must \\nselect the correct responses and place the responses in the correct order to \\nreceive credit for the question. \\n• Matching: Has a list of responses to match with a list of 3–7 prompts. You \\nmust match all the pairs correctly to receive credit for the question. \\n• Case study: Has one scenario with two or more questions about the scenario. \\nThe scenario is the same for each question in the case study. Each question in \\nthe case study will be evaluated separately. You will receive credit for each \\nquestion that you answer correctly in the case study. \\nVersion 1.4 AIF-C01 2 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='Unanswered questions are scored as incorrect; there is no penalty for guessing. The \\nexam includes 50 questions that affect your score.1 \\nUnscored content \\nThe exam includes 15 unscored questions that do not affect your score. AWS collects \\ninformation about performance on these unscored questions to evaluate these \\nquestions for future use as scored questions. These unscored questions are not \\nidentified on the exam. \\nExam results \\nThe AWS Certified AI Practitioner (AIF-C01) exam has a pass or fail designation. The \\nexam is scored against a minimum standard established by AWS professionals who \\nfollow certification industry best practices and guidelines. \\nYour results for the exam are reported as a scaled score of 100–1,000. The minimum \\npassing score is 700. Your score shows how you performed on the exam as a whole \\nand whether you passed. Scaled scoring models help equate scores across multiple \\nexam forms that might have slightly different difficulty levels. \\nYour score report could contain a table of classifications of your performance at each \\nsection level. The exam uses a compensatory scoring model, which means that you do \\nnot need to achieve a passing score in each section. You need to pass only the overall \\nexam. \\nEach section of the exam has a specific weighting, so some sections have more \\nquestions than other sections have. The table of classifications contains general \\ninformation that highlights your strengths and weaknesses. Use caution when you \\ninterpret section-level feedback. \\n1 Does not apply to the beta version of the exam. You can find more information about \\nbeta exams in general on the AWS Certification website. \\nVersion 1.4 AIF-C01 3 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='Content outline \\nThis exam guide includes weightings, content domains, and task statements for the \\nexam. This guide does not provide a comprehensive list of the content on the exam. \\nHowever, additional context for each task statement is available to help you prepare \\nfor the exam. \\nThe exam has the following content domains and weightings: \\n• Domain 1: Fundamentals of AI and ML (20% of scored content) \\n• Domain 2: Fundamentals of Generative AI (24% of scored content) \\n• Domain 3: Applications of Foundation Models (28% of scored content) \\n• Domain 4: Guidelines for Responsible AI (14% of scored content) \\n• Domain 5: Security, Compliance, and Governance for AI Solutions (14% of \\nscored content) \\nDomain 1: Fundamentals of AI and ML \\nTask Statement 1.1: Explain basic AI concepts and terminologies. \\nObjectives: \\n• Define basic AI terms (for example, AI, ML, deep learning, neural networks, \\ncomputer vision, natural language processing [NLP], model, algorithm, \\ntraining and inferencing, bias, fairness, fit, large language model [LLM]). \\n• Describe the similarities and differences between AI, ML, and deep learning. \\n• Describe various types of inferencing (for example, batch, real-time). \\n• Describe the different types of data in AI models (for example, labeled and \\nunlabeled, tabular, time-series, image, text, structured and unstructured). \\n• Describe supervised learning, unsupervised learning, and reinforcement \\nlearning. \\nVersion 1.4 AIF-C01 4 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='Task Statement 1.2: Identify practical use cases for AI. \\nObjectives: \\n• Recognize applications where AI/ML can provide value (for example, assist \\nhuman decision making, solution scalability, automation). \\n• Determine when AI/ML solutions are not appropriate (for example, cost-\\nbenefit analyses, situations when a specific outcome is needed instead of a \\nprediction). \\n• Select the appropriate ML techniques for specific use cases (for example, \\nregression, classification, clustering). \\n• Identify examples of real-world AI applications (for example, computer \\nvision, NLP, speech recognition, recommendation systems, fraud detection, \\nforecasting). \\n• Explain the capabilities of AWS managed AI/ML services (for example, \\nSageMaker, Amazon Transcribe, Amazon Translate, Amazon Comprehend, \\nAmazon Lex, Amazon Polly). \\nTask Statement 1.3: Describe the ML development lifecycle. \\nObjectives: \\n• Describe components of an ML pipeline (for example, data collection, \\nexploratory data analysis [EDA], data pre-processing, feature engineering, \\nmodel training, hyperparameter tuning, evaluation, deployment, \\nmonitoring). \\n• Understand sources of ML models (for example, open source pre-trained \\nmodels, training custom models). \\n• Describe methods to use a model in production (for example, managed API \\nservice, self-hosted API). \\n• Identify relevant AWS services and features for each stage of an ML pipeline \\n(for example, SageMaker, Amazon SageMaker Data Wrangler, Amazon \\nSageMaker Feature Store, Amazon SageMaker Model Monitor). \\nVersion 1.4 AIF-C01 5 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='• Understand fundamental concepts of ML operations (MLOps) (for example, \\nexperimentation, repeatable processes, scalable systems, managing \\ntechnical debt, achieving production readiness, model monitoring, model \\nre-training). \\n• Understand model performance metrics (for example, accuracy, Area Under \\nthe ROC Curve [AUC], F1 score) and business metrics (for example, cost per \\nuser, development costs, customer feedback, return on investment [ROI]) to \\nevaluate ML models. \\nDomain 2: Fundamentals of Generative AI \\nTask Statement 2.1: Explain the basic concepts of generative AI. \\nObjectives: \\n• Understand foundational generative AI concepts (for example, tokens, \\nchunking, embeddings, vectors, prompt engineering, transformer-based \\nLLMs, foundation models, multi-modal models, diffusion models). \\n• Identify potential use cases for generative AI models (for example, image, \\nvideo, and audio generation; summarization; chatbots; translation; code \\ngeneration; customer service agents; search; recommendation engines). \\n• Describe the foundation model lifecycle (for example, data selection, model \\nselection, pre-training, fine-tuning, evaluation, deployment, feedback). \\nTask Statement 2.2: Understand the capabilities and limitations of generative AI for \\nsolving business problems. \\nObjectives: \\n• Describe the advantages of generative AI (for example, adaptability, \\nresponsiveness, simplicity). \\n• Identify disadvantages of generative AI solutions (for example, \\nhallucinations, interpretability, inaccuracy, nondeterminism). \\n• Understand various factors to select appropriate generative AI models (for \\nexample, model types, performance requirements, capabilities, constraints, \\ncompliance). \\n• Determine business value and metrics for generative AI applications (for \\nexample, cross-domain performance, efficiency, conversion rate, average \\nrevenue per user, accuracy, customer lifetime value). \\nVersion 1.4 AIF-C01 6 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='Task Statement 2.3: Describe AWS infrastructure and technologies for building \\ngenerative AI applications. \\nObjectives: \\n• Identify AWS services and features to develop generative AI applications \\n(for example, Amazon SageMaker JumpStart; Amazon Bedrock; PartyRock, \\nan Amazon Bedrock Playground; Amazon Q). \\n• Describe the advantages of using AWS generative AI services to build \\napplications (for example, accessibility, lower barrier to entry, efficiency, \\ncost-effectiveness, speed to market, ability to meet business objectives). \\n• Understand the benefits of AWS infrastructure for generative AI \\napplications (for example, security, compliance, responsibility, safety). \\n• Understand cost tradeoffs of AWS generative AI services (for example, \\nresponsiveness, availability, redundancy, performance, regional coverage, \\ntoken-based pricing, provision throughput, custom models). \\nDomain 3: Applications of Foundation Models \\nTask Statement 3.1: Describe design considerations for applications that use \\nfoundation models. \\nObjectives: \\n• Identify selection criteria to choose pre-trained models (for example, cost, \\nmodality, latency, multi-lingual, model size, model complexity, \\ncustomization, input/output length). \\n• Understand the effect of inference parameters on model responses (for \\nexample, temperature, input/output length). \\n• Define Retrieval Augmented Generation (RAG) and describe its business \\napplications (for example, Amazon Bedrock, knowledge base). \\n• Identify AWS services that help store embeddings within vector databases \\n(for example, Amazon OpenSearch Service, Amazon Aurora, Amazon \\nNeptune, Amazon DocumentDB [with MongoDB compatibility], Amazon \\nRDS for PostgreSQL). \\nVersion 1.4 AIF-C01 7 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='• Explain the cost tradeoffs of various approaches to foundation model \\ncustomization (for example, pre-training, fine-tuning, in-context learning, \\nRAG). \\n• Understand the role of agents in multi-step tasks (for example, Agents for \\nAmazon Bedrock). \\nTask Statement 3.2: Choose effective prompt engineering techniques. \\nObjectives: \\n• Describe the concepts and constructs of prompt engineering (for example, \\ncontext, instruction, negative prompts, model latent space). \\n• Understand techniques for prompt engineering (for example, chain-of-\\nthought, zero-shot, single-shot, few-shot, prompt templates). \\n• Understand the benefits and best practices for prompt engineering (for \\nexample, response quality improvement, experimentation, guardrails, \\ndiscovery, specificity and concision, using multiple comments). \\n• Define potential risks and limitations of prompt engineering (for example, \\nexposure, poisoning, hijacking, jailbreaking). \\nTask Statement 3.3: Describe the training and fine-tuning process for foundation \\nmodels. \\nObjectives: \\n• Describe the key elements of training a foundation model (for example, \\npre-training, fine-tuning, continuous pre-training). \\n• Define methods for fine-tuning a foundation model (for example, \\ninstruction tuning, adapting models for specific domains, transfer learning, \\ncontinuous pre-training). \\n• Describe how to prepare data to fine-tune a foundation model (for \\nexample, data curation, governance, size, labeling, representativeness, \\nreinforcement learning from human feedback [RLHF]). \\nVersion 1.4 AIF-C01 8 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='Task Statement 3.4: Describe methods to evaluate foundation model performance. \\nObjectives: \\n• Understand approaches to evaluate foundation model performance (for \\nexample, human evaluation, benchmark datasets). \\n• Identify relevant metrics to assess foundation model performance (for \\nexample, Recall-Oriented Understudy for Gisting Evaluation [ROUGE], \\nBilingual Evaluation Understudy [BLEU], BERTScore). \\n• Determine whether a foundation model effectively meets business \\nobjectives (for example, productivity, user engagement, task engineering). \\nDomain 4: Guidelines for Responsible AI \\nTask Statement 4.1: Explain the development of AI systems that are responsible. \\nObjectives: \\n• Identify features of responsible AI (for example, bias, fairness, inclusivity, \\nrobustness, safety, veracity). \\n• Understand how to use tools to identify features of responsible AI (for \\nexample, Guardrails for Amazon Bedrock). \\n• Understand responsible practices to select a model (for example, \\nenvironmental considerations, sustainability). \\n• Identify legal risks of working with generative AI (for example, intellectual \\nproperty infringement claims, biased model outputs, loss of customer trust, \\nend user risk, hallucinations). \\n• Identify characteristics of datasets (for example, inclusivity, diversity, \\ncurated data sources, balanced datasets). \\n• Understand effects of bias and variance (for example, effects on \\ndemographic groups, inaccuracy, overfitting, underfitting). \\n• Describe tools to detect and monitor bias, trustworthiness, and truthfulness \\n(for example, analyzing label quality, human audits, subgroup analysis, \\nAmazon SageMaker Clarify, SageMaker Model Monitor, Amazon Augmented \\nAI [Amazon A2I]). \\nVersion 1.4 AIF-C01 9 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='Task Statement 4.2: Recognize the importance of transparent and explainable \\nmodels. \\nObjectives: \\n• Understand the differences between models that are transparent and \\nexplainable and models that are not transparent and explainable. \\n• Understand the tools to identify transparent and explainable models (for \\nexample, Amazon SageMaker Model Cards, open source models, data, \\nlicensing). \\n• Identify tradeoffs between model safety and transparency (for example, \\nmeasure interpretability and performance). \\n• Understand principles of human-centered design for explainable AI. \\nDomain 5: Security, Compliance, and Governance for AI Solutions \\nTask Statement 5.1: Explain methods to secure AI systems. \\nObjectives: \\n• Identify AWS services and features to secure AI systems (for example, IAM \\nroles, policies, and permissions; encryption; Amazon Macie; AWS \\nPrivateLink; AWS shared responsibility model). \\n• Understand the concept of source citation and documenting data origins \\n(for example, data lineage, data cataloging, SageMaker Model Cards). \\n• Describe best practices for secure data engineering (for example, assessing \\ndata quality, implementing privacy-enhancing technologies, data access \\ncontrol, data integrity). \\n• Understand security and privacy considerations for AI systems (for example, \\napplication security, threat detection, vulnerability management, \\ninfrastructure protection, prompt injection, encryption at rest and in \\ntransit). \\nVersion 1.4 AIF-C01 10 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='Task Statement 5.2: Recognize governance and compliance regulations for AI \\nsystems. \\nObjectives: \\n• Identify regulatory compliance standards for AI systems (for example, \\nInternational Organization for Standardization [ISO], System and \\nOrganization Controls [SOC], algorithm accountability laws). \\n• Identify AWS services and features to assist with governance and regulation \\ncompliance (for example, AWS Config, Amazon Inspector, AWS Audit \\nManager, AWS Artifact, AWS CloudTrail, AWS Trusted Advisor). \\n• Describe data governance strategies (for example, data lifecycles, logging, \\nresidency, monitoring, observation, retention). \\n• Describe processes to follow governance protocols (for example, policies, \\nreview cadence, review strategies, governance frameworks such as the \\nGenerative AI Security Scoping Matrix, transparency standards, team \\ntraining requirements). \\nVersion 1.4 AIF-C01 11 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='Appendix \\nIn-scope AWS services and features \\nThe following list contains AWS services and features that are in scope for the exam. \\nThis list is non-exhaustive and is subject to change. AWS offerings appear in \\ncategories that align with the offerings’ primary functions: \\nAnalytics: \\n• AWS Data Exchange \\n• Amazon EMR \\n• AWS Glue \\n• AWS Glue DataBrew \\n• AWS Lake Formation \\n• Amazon OpenSearch Service \\n• Amazon QuickSight \\n• Amazon Redshift \\nCloud Financial Management: \\n• AWS Budgets \\n• AWS Cost Explorer \\nCompute: \\n• Amazon EC2 \\nContainers: \\n• Amazon Elastic Container Service (Amazon ECS) \\n• Amazon Elastic Kubernetes Service (Amazon EKS) \\nVersion 1.4 AIF-C01 12 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='Database: \\n• Amazon DocumentDB (with MongoDB compatibility) \\n• Amazon DynamoDB \\n• Amazon ElastiCache \\n• Amazon MemoryDB \\n• Amazon Neptune \\n• Amazon RDS \\nMachine Learning: \\n• Amazon Augmented AI (Amazon A2I) \\n• Amazon Bedrock \\n• Amazon Comprehend \\n• Amazon Fraud Detector \\n• Amazon Kendra \\n• Amazon Lex \\n• Amazon Personalize \\n• Amazon Polly \\n• Amazon Q \\n• Amazon Rekognition \\n• Amazon SageMaker \\n• Amazon Textract \\n• Amazon Transcribe \\n• Amazon Translate \\nManagement and Governance: \\n• AWS CloudTrail \\n• Amazon CloudWatch \\n• AWS Config \\n• AWS Trusted Advisor \\n• AWS Well-Architected Tool \\nVersion 1.4 AIF-C01 13 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='Networking and Content Delivery: \\n• Amazon CloudFront \\n• Amazon VPC \\nSecurity, Identity, and Compliance: \\n• AWS Artifact \\n• AWS Audit Manager \\n• AWS Identity and Access Management (IAM) \\n• Amazon Inspector \\n• AWS Key Management Service (AWS KMS) \\n• Amazon Macie \\n• AWS Secrets Manager \\nStorage: \\n• Amazon S3 \\n• Amazon S3 Glacier \\nOut-of-scope AWS services and features \\nThe following list contains AWS services and features that are out of scope for the \\nexam. This list is non-exhaustive and is subject to change. AWS offerings that are \\nentirely unrelated to the target job roles for the exam are excluded from this list: \\nAnalytics: \\n• AWS Clean Rooms \\n• Amazon CloudSearch \\n• Amazon FinSpace \\n• Amazon Managed Streaming for Apache Kafka (Amazon MSK) \\nApplication Integration: \\n• Amazon AppFlow \\n• Amazon MQ \\n• Amazon Simple Workflow Service (Amazon SWF) \\nVersion 1.4 AIF-C01 14 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='Business Applications: \\n• Amazon Chime \\n• Amazon Honeycode \\n• Amazon Pinpoint \\n• Amazon Simple Email Service (Amazon SES) \\n• AWS Supply Chain \\n• AWS Wickr \\n• Amazon WorkDocs \\n• Amazon WorkMail \\nCloud Financial Management: \\n• AWS Application Cost Profiler \\n• AWS Billing Conductor \\n• AWS Marketplace \\nCompute: \\n• AWS App Runner \\n• AWS Elastic Beanstalk \\n• EC2 Image Builder \\n• Amazon Lightsail \\nContainers: \\n• Red Hat OpenShift Service on AWS (ROSA) \\nCustomer Enablement: \\n• AWS IQ \\n• AWS Managed Services (AMS) \\n• AWS re:Post Private \\n• AWS Support \\nVersion 1.4 AIF-C01 15 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16'}, page_content='Database: \\n• Amazon Keyspaces (for Apache Cassandra) \\n• Amazon Quantum Ledger Database (Amazon QLDB) \\n• Amazon Timestream \\nDeveloper Tools: \\n• AWS AppConfig \\n• AWS Application Composer \\n• AWS CloudShell \\n• Amazon CodeCatalyst \\n• AWS CodeStar \\n• AWS Fault Injection Service \\n• AWS X-Ray \\nEnd User Computing: \\n• Amazon AppStream 2.0 \\n• Amazon WorkSpaces \\n• Amazon WorkSpaces Thin Client \\n• Amazon WorkSpaces Web \\nFrontend Web and Mobile: \\n• AWS Amplify \\n• AWS AppSync \\n• AWS Device Farm \\n• Amazon Location Service \\nInternet of Things (IoT): \\n• AWS IoT Analytics \\n• AWS IoT Core \\n• AWS IoT Device Defender \\n• AWS IoT Device Management \\n• AWS IoT Events \\n• AWS IoT FleetWise \\n• FreeRTOS \\nVersion 1.4 AIF-C01 16 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='• AWS IoT Greengrass \\n• AWS IoT 1-Click \\n• AWS IoT RoboRunner \\n• AWS IoT SiteWise \\n• AWS IoT TwinMaker \\nMachine Learning: \\n• AWS DeepComposer \\n• AWS HealthImaging \\n• AWS HealthOmics \\n• Amazon Monitron \\n• AWS Panorama \\nManagement and Governance: \\n• AWS Control Tower \\n• AWS Health Dashboard \\n• AWS Launch Wizard \\n• AWS License Manager \\n• Amazon Managed Grafana \\n• Amazon Managed Service for Prometheus \\n• AWS OpsWorks \\n• AWS Organizations \\n• AWS Proton \\n• AWS Resilience Hub \\n• AWS Resource Explorer \\n• AWS Resource Groups \\n• AWS Systems Manager Incident Manager \\n• AWS Service Catalog \\n• Service Quotas \\n• AWS Telco Network Builder \\n• AWS User Notifications \\nVersion 1.4 AIF-C01 17 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='Media: \\n• Amazon Elastic Transcoder \\n• AWS Elemental MediaConnect \\n• AWS Elemental MediaConvert \\n• AWS Elemental MediaLive \\n• AWS Elemental MediaPackage \\n• AWS Elemental MediaStore \\n• AWS Elemental MediaTailor \\n• Amazon Interactive Video Service (Amazon IVS) \\n• Amazon Nimble Studio \\nMigration and Transfer: \\n• AWS Application Discovery Service \\n• AWS Application Migration Service \\n• AWS Database Migration Service (AWS DMS) \\n• AWS DataSync \\n• AWS Mainframe Modernization \\n• AWS Migration Hub \\n• AWS Snow Family \\n• AWS Transfer Family \\nNetworking and Content Delivery: \\n• AWS App Mesh \\n• AWS Cloud Map \\n• AWS Direct Connect \\n• AWS Global Accelerator \\n• AWS Private 5G \\n• Amazon Route 53 \\n• Amazon Route 53 Application Recovery Controller \\n• Amazon VPC IP Address Manager (IPAM) \\nVersion 1.4 AIF-C01 18 | PAGE'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-07-31T13:52:58-04:00', 'author': 'Barnosky, Timothy', 'moddate': '2025-02-11T11:40:25-07:00', 'title': 'AWS Certified AI Practitioner (AIF-C01) Exam Guide', 'source': 'AWS-Certified-AI-Practitioner_Exam-Guide.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='Security, Identity, and Compliance: \\n• AWS Certificate Manager (ACM) \\n• AWS CloudHSM \\n• Amazon Cognito \\n• Amazon Detective \\n• AWS Directory Service \\n• AWS Firewall Manager \\n• Amazon GuardDuty \\n• AWS IAM Identity Center \\n• AWS Payment Cryptography \\n• AWS Private Certificate Authority \\n• AWS Resource Access Manager (AWS RAM) \\n• AWS Security Hub \\n• Amazon Security Lake \\n• AWS Shield \\n• AWS Signer \\n• Amazon Verified Permissions \\n• AWS WAF \\nStorage: \\n• AWS Backup \\n• AWS Elastic Disaster Recovery \\nSurvey \\nHow useful was this exam guide? Let us know by taking our survey. \\nVersion 1.4 AIF-C01 19 | PAGE')]\n"
     ]
    }
   ],
   "source": [
    "#PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(file_path=\"AWS-Certified-AI-Practitioner_Exam-Guide.pdf\")\n",
    "docu = loader.load()\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nNlp\\nLanguage-Model\\nAgent\\nSteerability\\nPrompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#HTML\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",)\n",
    "docu=loader.load()\n",
    "print(docu)\n",
    "print(len(docu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd22a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 1}, page_content='{\"Skip to main content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 2}, page_content='{\"Skip to main content\": \">cs> arXiv:1706.03762 \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 3}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 4}, page_content='{\"Skip to main content\": \"Help | Advanced Search\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 5}, page_content='{\"Skip to main content\": \"All fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 6}, page_content='{\"Skip to main content\": \"Search\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 7}, page_content='{\"Skip to main content\": \"GO\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 8}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 9}, page_content='{\"Skip to main content\": \"quick links\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 10}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 11}, page_content='{\"Skip to main content\": \"Login\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 12}, page_content='{\"Skip to main content\": \"Help Pages\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 13}, page_content='{\"Skip to main content\": \"About\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 14}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 15}, page_content='{\"Skip to main content\": \"Computer Science > Computation and Language\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 16}, page_content='{\"Skip to main content\": \"arXiv:1706.03762 (cs) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 17}, page_content='{\"Skip to main content\": \"[Submitted on 12 Jun 2017 (v1), last revised 2 Aug 2023 (this version, v7)]\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 18}, page_content='{\"Skip to main content\": \"Title:Attention Is All You Need\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 19}, page_content='{\"Skip to main content\": \"Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 20}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 21}, page_content='{\"Skip to main content\": \"View PDF    HTML (experimental)                Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 22}, page_content='{\"Skip to main content\": \"Comments:\", \"Column2\": \"15 pages, 5 figures\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 23}, page_content='{\"Skip to main content\": \"Subjects:\", \"Column2\": \"Computation and Language (cs.CL); Machine Learning (cs.LG)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 24}, page_content='{\"Skip to main content\": \"Cite as:\", \"Column2\": \"arXiv:1706.03762 [cs.CL]\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 25}, page_content='{\"Skip to main content\": \" \", \"Column2\": \"(or               arXiv:1706.03762v7 [cs.CL] for this version) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 26}, page_content='{\"Skip to main content\": \" \", \"Column2\": \"https://doi.org/10.48550/arXiv.1706.03762                                              Focus to learn more                                                                            arXiv-issued DOI via DataCite \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 27}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 28}, page_content='{\"Skip to main content\": \"Submission history\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 29}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 30}, page_content='{\"Skip to main content\": \"From: Llion Jones [view email] \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 31}, page_content='{\"Skip to main content\": \"[v1]        Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 32}, page_content='{\"Skip to main content\": \"[v2]        Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 33}, page_content='{\"Skip to main content\": \"[v3]        Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 34}, page_content='{\"Skip to main content\": \"[v4]        Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 35}, page_content='{\"Skip to main content\": \"[v5]        Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 36}, page_content='{\"Skip to main content\": \"[v6]        Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 37}, page_content='{\"Skip to main content\": \"[v7]        Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 38}, page_content='{\"Skip to main content\": \"Full-text links: \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 39}, page_content='{\"Skip to main content\": \"Access Paper:\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 40}, page_content='{\"Skip to main content\": \"View a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 41}, page_content='{\"Skip to main content\": \"View PDF\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 42}, page_content='{\"Skip to main content\": \"HTML (experimental)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 43}, page_content='{\"Skip to main content\": \"TeX Source \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 44}, page_content='{\"Skip to main content\": \"view license\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 45}, page_content='{\"Skip to main content\": \"Current browse context: cs.CL\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 46}, page_content='{\"Skip to main content\": \"< prev          |            next > \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 47}, page_content='{\"Skip to main content\": \"new     |     recent     | 2017-06 \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 48}, page_content='{\"Skip to main content\": \"Change to browse by:         cs\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 49}, page_content='{\"Skip to main content\": \"cs.LG\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 50}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 51}, page_content='{\"Skip to main content\": \"References & Citations\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 52}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 53}, page_content='{\"Skip to main content\": \"NASA ADS\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 54}, page_content='{\"Skip to main content\": \"Google Scholar\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 55}, page_content='{\"Skip to main content\": \"Semantic Scholar\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 56}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 57}, page_content='{\"Skip to main content\": \"123 blog links\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 58}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 59}, page_content='{\"Skip to main content\": \"(what is this?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 60}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 61}, page_content='{\"Skip to main content\": \"DBLP - CS Bibliography\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 62}, page_content='{\"Skip to main content\": \"listing | bibtex \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 63}, page_content='{\"Skip to main content\": \"Ashish Vaswani\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 64}, page_content='{\"Skip to main content\": \"Noam Shazeer\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 65}, page_content='{\"Skip to main content\": \"Niki Parmar\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 66}, page_content='{\"Skip to main content\": \"Jakob Uszkoreit\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 67}, page_content='{\"Skip to main content\": \"Llion Jones      \\\\u2026\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 68}, page_content='{\"Skip to main content\": \"export BibTeX citation    Loading...\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 69}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 70}, page_content='{\"Skip to main content\": \"BibTeX formatted citation\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 71}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 72}, page_content='{\"Skip to main content\": \"\\\\u00d7 \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 73}, page_content='{\"Skip to main content\": \"loading... \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 74}, page_content='{\"Skip to main content\": \"Data provided by: \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 75}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 76}, page_content='{\"Skip to main content\": \"Bookmark\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 77}, page_content='{\"Skip to main content\": \"Bibliographic Tools \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 78}, page_content='{\"Skip to main content\": \"Bibliographic and Citation Tools\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 79}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 80}, page_content='{\"Skip to main content\": \"Bibliographic Explorer Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 81}, page_content='{\"Skip to main content\": \"Bibliographic Explorer (What is the Explorer?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 82}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 83}, page_content='{\"Skip to main content\": \"Connected Papers Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 84}, page_content='{\"Skip to main content\": \"Connected Papers (What is Connected Papers?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 85}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 86}, page_content='{\"Skip to main content\": \"Litmaps Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 87}, page_content='{\"Skip to main content\": \"Litmaps (What is Litmaps?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 88}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 89}, page_content='{\"Skip to main content\": \"scite.ai Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 90}, page_content='{\"Skip to main content\": \"scite Smart Citations (What are Smart Citations?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 91}, page_content='{\"Skip to main content\": \"Code, Data, Media \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 92}, page_content='{\"Skip to main content\": \"Code, Data and Media Associated with this Article\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 93}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 94}, page_content='{\"Skip to main content\": \"alphaXiv Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 95}, page_content='{\"Skip to main content\": \"alphaXiv (What is alphaXiv?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 96}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 97}, page_content='{\"Skip to main content\": \"Links to Code Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 98}, page_content='{\"Skip to main content\": \"CatalyzeX Code Finder for Papers (What is CatalyzeX?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 99}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 100}, page_content='{\"Skip to main content\": \"DagsHub Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 101}, page_content='{\"Skip to main content\": \"DagsHub (What is DagsHub?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 102}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 103}, page_content='{\"Skip to main content\": \"GotitPub Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 104}, page_content='{\"Skip to main content\": \"Gotit.pub (What is GotitPub?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 105}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 106}, page_content='{\"Skip to main content\": \"Huggingface Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 107}, page_content='{\"Skip to main content\": \"Hugging Face (What is Huggingface?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 108}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 109}, page_content='{\"Skip to main content\": \"Links to Code Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 110}, page_content='{\"Skip to main content\": \"Papers with Code (What is Papers with Code?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 111}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 112}, page_content='{\"Skip to main content\": \"ScienceCast Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 113}, page_content='{\"Skip to main content\": \"ScienceCast (What is ScienceCast?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 114}, page_content='{\"Skip to main content\": \"Demos \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 115}, page_content='{\"Skip to main content\": \"Demos\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 116}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 117}, page_content='{\"Skip to main content\": \"Replicate Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 118}, page_content='{\"Skip to main content\": \"Replicate (What is Replicate?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 119}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 120}, page_content='{\"Skip to main content\": \"Spaces Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 121}, page_content='{\"Skip to main content\": \"Hugging Face Spaces (What is Spaces?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 122}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 123}, page_content='{\"Skip to main content\": \"Spaces Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 124}, page_content='{\"Skip to main content\": \"TXYZ.AI (What is TXYZ.AI?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 125}, page_content='{\"Skip to main content\": \"Related Papers \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 126}, page_content='{\"Skip to main content\": \"Recommenders and Search Tools\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 127}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 128}, page_content='{\"Skip to main content\": \"Link to Influence Flower \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 129}, page_content='{\"Skip to main content\": \"Influence Flower (What are Influence Flowers?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 130}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 131}, page_content='{\"Skip to main content\": \"Core recommender toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 132}, page_content='{\"Skip to main content\": \"CORE Recommender (What is CORE?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 133}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 134}, page_content='{\"Skip to main content\": \"Author\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 135}, page_content='{\"Skip to main content\": \"Venue\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 136}, page_content='{\"Skip to main content\": \"Institution\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 137}, page_content='{\"Skip to main content\": \"Topic\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 138}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 139}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 140}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 141}, page_content='{\"Skip to main content\": \"About arXivLabs \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 142}, page_content='{\"Skip to main content\": \"arXivLabs: experimental projects with community collaborators\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 143}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 144}, page_content='{\"Skip to main content\": \"arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 145}, page_content='{\"Skip to main content\": \"Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 146}, page_content='{\"Skip to main content\": \"Have an idea for a project that will add value for arXiv\\'s community? Learn more about arXivLabs.\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 147}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 148}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 149}, page_content='{\"Skip to main content\": \"Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 150}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 151}, page_content='{\"Skip to main content\": \"About\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 152}, page_content='{\"Skip to main content\": \"Help\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 153}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 154}, page_content='{\"Skip to main content\": \"Contact \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 155}, page_content='{\"Skip to main content\": \"Subscribe \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 156}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 157}, page_content='{\"Skip to main content\": \"Copyright\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 158}, page_content='{\"Skip to main content\": \"Privacy Policy\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 159}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 160}, page_content='{\"Skip to main content\": \"Web Accessibility Assistance\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 161}, page_content='{\"Skip to main content\": \"arXiv Operational Status \"}')]\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "[Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 1}, page_content='{\"Skip to main content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 2}, page_content='{\"Skip to main content\": \">cs> arXiv:1706.03762 \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 3}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 4}, page_content='{\"Skip to main content\": \"Help | Advanced Search\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 5}, page_content='{\"Skip to main content\": \"All fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 6}, page_content='{\"Skip to main content\": \"Search\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 7}, page_content='{\"Skip to main content\": \"GO\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 8}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 9}, page_content='{\"Skip to main content\": \"quick links\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 10}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 11}, page_content='{\"Skip to main content\": \"Login\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 12}, page_content='{\"Skip to main content\": \"Help Pages\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 13}, page_content='{\"Skip to main content\": \"About\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 14}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 15}, page_content='{\"Skip to main content\": \"Computer Science > Computation and Language\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 16}, page_content='{\"Skip to main content\": \"arXiv:1706.03762 (cs) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 17}, page_content='{\"Skip to main content\": \"[Submitted on 12 Jun 2017 (v1), last revised 2 Aug 2023 (this version, v7)]\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 18}, page_content='{\"Skip to main content\": \"Title:Attention Is All You Need\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 19}, page_content='{\"Skip to main content\": \"Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 20}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 21}, page_content='{\"Skip to main content\": \"View PDF    HTML (experimental)                Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 22}, page_content='{\"Skip to main content\": \"Comments:\", \"Column2\": \"15 pages, 5 figures\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 23}, page_content='{\"Skip to main content\": \"Subjects:\", \"Column2\": \"Computation and Language (cs.CL); Machine Learning (cs.LG)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 24}, page_content='{\"Skip to main content\": \"Cite as:\", \"Column2\": \"arXiv:1706.03762 [cs.CL]\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 25}, page_content='{\"Skip to main content\": \" \", \"Column2\": \"(or               arXiv:1706.03762v7 [cs.CL] for this version) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 26}, page_content='{\"Skip to main content\": \" \", \"Column2\": \"https://doi.org/10.48550/arXiv.1706.03762                                              Focus to learn more                                                                            arXiv-issued DOI via DataCite \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 27}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 28}, page_content='{\"Skip to main content\": \"Submission history\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 29}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 30}, page_content='{\"Skip to main content\": \"From: Llion Jones [view email] \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 31}, page_content='{\"Skip to main content\": \"[v1]        Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 32}, page_content='{\"Skip to main content\": \"[v2]        Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 33}, page_content='{\"Skip to main content\": \"[v3]        Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 34}, page_content='{\"Skip to main content\": \"[v4]        Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 35}, page_content='{\"Skip to main content\": \"[v5]        Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 36}, page_content='{\"Skip to main content\": \"[v6]        Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 37}, page_content='{\"Skip to main content\": \"[v7]        Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 38}, page_content='{\"Skip to main content\": \"Full-text links: \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 39}, page_content='{\"Skip to main content\": \"Access Paper:\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 40}, page_content='{\"Skip to main content\": \"View a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 41}, page_content='{\"Skip to main content\": \"View PDF\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 42}, page_content='{\"Skip to main content\": \"HTML (experimental)\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 43}, page_content='{\"Skip to main content\": \"TeX Source \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 44}, page_content='{\"Skip to main content\": \"view license\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 45}, page_content='{\"Skip to main content\": \"Current browse context: cs.CL\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 46}, page_content='{\"Skip to main content\": \"< prev          |            next > \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 47}, page_content='{\"Skip to main content\": \"new     |     recent     | 2017-06 \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 48}, page_content='{\"Skip to main content\": \"Change to browse by:         cs\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 49}, page_content='{\"Skip to main content\": \"cs.LG\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 50}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 51}, page_content='{\"Skip to main content\": \"References & Citations\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 52}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 53}, page_content='{\"Skip to main content\": \"NASA ADS\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 54}, page_content='{\"Skip to main content\": \"Google Scholar\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 55}, page_content='{\"Skip to main content\": \"Semantic Scholar\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 56}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 57}, page_content='{\"Skip to main content\": \"123 blog links\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 58}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 59}, page_content='{\"Skip to main content\": \"(what is this?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 60}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 61}, page_content='{\"Skip to main content\": \"DBLP - CS Bibliography\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 62}, page_content='{\"Skip to main content\": \"listing | bibtex \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 63}, page_content='{\"Skip to main content\": \"Ashish Vaswani\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 64}, page_content='{\"Skip to main content\": \"Noam Shazeer\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 65}, page_content='{\"Skip to main content\": \"Niki Parmar\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 66}, page_content='{\"Skip to main content\": \"Jakob Uszkoreit\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 67}, page_content='{\"Skip to main content\": \"Llion Jones      \\\\u2026\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 68}, page_content='{\"Skip to main content\": \"export BibTeX citation    Loading...\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 69}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 70}, page_content='{\"Skip to main content\": \"BibTeX formatted citation\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 71}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 72}, page_content='{\"Skip to main content\": \"\\\\u00d7 \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 73}, page_content='{\"Skip to main content\": \"loading... \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 74}, page_content='{\"Skip to main content\": \"Data provided by: \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 75}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 76}, page_content='{\"Skip to main content\": \"Bookmark\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 77}, page_content='{\"Skip to main content\": \"Bibliographic Tools \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 78}, page_content='{\"Skip to main content\": \"Bibliographic and Citation Tools\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 79}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 80}, page_content='{\"Skip to main content\": \"Bibliographic Explorer Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 81}, page_content='{\"Skip to main content\": \"Bibliographic Explorer (What is the Explorer?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 82}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 83}, page_content='{\"Skip to main content\": \"Connected Papers Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 84}, page_content='{\"Skip to main content\": \"Connected Papers (What is Connected Papers?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 85}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 86}, page_content='{\"Skip to main content\": \"Litmaps Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 87}, page_content='{\"Skip to main content\": \"Litmaps (What is Litmaps?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 88}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 89}, page_content='{\"Skip to main content\": \"scite.ai Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 90}, page_content='{\"Skip to main content\": \"scite Smart Citations (What are Smart Citations?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 91}, page_content='{\"Skip to main content\": \"Code, Data, Media \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 92}, page_content='{\"Skip to main content\": \"Code, Data and Media Associated with this Article\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 93}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 94}, page_content='{\"Skip to main content\": \"alphaXiv Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 95}, page_content='{\"Skip to main content\": \"alphaXiv (What is alphaXiv?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 96}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 97}, page_content='{\"Skip to main content\": \"Links to Code Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 98}, page_content='{\"Skip to main content\": \"CatalyzeX Code Finder for Papers (What is CatalyzeX?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 99}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 100}, page_content='{\"Skip to main content\": \"DagsHub Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 101}, page_content='{\"Skip to main content\": \"DagsHub (What is DagsHub?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 102}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 103}, page_content='{\"Skip to main content\": \"GotitPub Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 104}, page_content='{\"Skip to main content\": \"Gotit.pub (What is GotitPub?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 105}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 106}, page_content='{\"Skip to main content\": \"Huggingface Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 107}, page_content='{\"Skip to main content\": \"Hugging Face (What is Huggingface?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 108}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 109}, page_content='{\"Skip to main content\": \"Links to Code Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 110}, page_content='{\"Skip to main content\": \"Papers with Code (What is Papers with Code?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 111}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 112}, page_content='{\"Skip to main content\": \"ScienceCast Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 113}, page_content='{\"Skip to main content\": \"ScienceCast (What is ScienceCast?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 114}, page_content='{\"Skip to main content\": \"Demos \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 115}, page_content='{\"Skip to main content\": \"Demos\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 116}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 117}, page_content='{\"Skip to main content\": \"Replicate Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 118}, page_content='{\"Skip to main content\": \"Replicate (What is Replicate?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 119}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 120}, page_content='{\"Skip to main content\": \"Spaces Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 121}, page_content='{\"Skip to main content\": \"Hugging Face Spaces (What is Spaces?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 122}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 123}, page_content='{\"Skip to main content\": \"Spaces Toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 124}, page_content='{\"Skip to main content\": \"TXYZ.AI (What is TXYZ.AI?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 125}, page_content='{\"Skip to main content\": \"Related Papers \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 126}, page_content='{\"Skip to main content\": \"Recommenders and Search Tools\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 127}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 128}, page_content='{\"Skip to main content\": \"Link to Influence Flower \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 129}, page_content='{\"Skip to main content\": \"Influence Flower (What are Influence Flowers?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 130}, page_content='{\"Skip to main content\": false}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 131}, page_content='{\"Skip to main content\": \"Core recommender toggle \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 132}, page_content='{\"Skip to main content\": \"CORE Recommender (What is CORE?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 133}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 134}, page_content='{\"Skip to main content\": \"Author\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 135}, page_content='{\"Skip to main content\": \"Venue\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 136}, page_content='{\"Skip to main content\": \"Institution\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 137}, page_content='{\"Skip to main content\": \"Topic\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 138}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 139}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 140}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 141}, page_content='{\"Skip to main content\": \"About arXivLabs \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 142}, page_content='{\"Skip to main content\": \"arXivLabs: experimental projects with community collaborators\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 143}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 144}, page_content='{\"Skip to main content\": \"arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 145}, page_content='{\"Skip to main content\": \"Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 146}, page_content='{\"Skip to main content\": \"Have an idea for a project that will add value for arXiv\\'s community? Learn more about arXivLabs.\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 147}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 148}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 149}, page_content='{\"Skip to main content\": \"Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 150}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 151}, page_content='{\"Skip to main content\": \"About\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 152}, page_content='{\"Skip to main content\": \"Help\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 153}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 154}, page_content='{\"Skip to main content\": \"Contact \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 155}, page_content='{\"Skip to main content\": \"Subscribe \"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 156}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 157}, page_content='{\"Skip to main content\": \"Copyright\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 158}, page_content='{\"Skip to main content\": \"Privacy Policy\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 159}, page_content=''), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 160}, page_content='{\"Skip to main content\": \"Web Accessibility Assistance\"}'), Document(metadata={'source': 'C:\\\\Users\\\\udaya\\\\OneDrive\\\\Desktop\\\\Langchain&Langgraph\\\\Langchain_Hands_On\\\\Data_Ingestion\\\\1706.json', 'seq_num': 161}, page_content='{\"Skip to main content\": \"arXiv Operational Status \"}')]\n"
     ]
    }
   ],
   "source": [
    "#JSON\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "loader=JSONLoader(file_path=\"1706.json\",jq_schema=\".[]\",text_content=False)\n",
    "docu=loader.load()\n",
    "print(docu)\n",
    "print(type(docu[0]))\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Repository: langchain\n",
      "    Description: 🦜🔗 The platform for reliable agents.\n",
      "    Stars: 124477\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#API\n",
    "import requests\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "response = requests.get(\"https://api.github.com/repos/langchain-ai/langchain\")\n",
    "data = response.json()\n",
    "\n",
    "doc = Document(\n",
    "    page_content=f\"\"\"\n",
    "    Repository: {data['name']}\n",
    "    Description: {data['description']}\n",
    "    Stars: {data['stargazers_count']}\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"GitHub API\"}\n",
    ")\n",
    "\n",
    "print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418869d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#WIKIPEDIA\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs=WikipediaLoader(query=\"Generative AI\",load_max_docs=3).load()\n",
    "#print(docs)\n",
    "#print(docs[0].page_content)\n",
    "#print(docs[1].page_content)\n",
    "print(len(docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
