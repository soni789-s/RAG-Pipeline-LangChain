Project Overview

This project demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline using LangChain and Ollama.
The system ingests data from multiple sources, splits it into meaningful chunks, generates embeddings locally using Ollama, and prepares the data for semantic search and question answering.

The main goal of this project is to understand and implement:

Data ingestion

Text chunking strategies

Local embedding generation using Ollama

Foundations of RAG systems

ğŸš€ Features

ğŸ“¥ Data ingestion from multiple sources (PDF, TXT, HTML, JSON, APIs)

âœ‚ï¸ Intelligent text chunking using LangChain splitters

ğŸ”¢ Local embedding generation using Ollama (nomic-embed-text)

ğŸ—„ï¸ Ready for vector database integration (FAISS / Chroma)

ğŸ”’ Fully offline and privacy-friendly (no cloud APIs required)

ğŸ› ï¸ Tech Stack

Python 3.11

LangChain

LangChain Community

Ollama (Local LLM & Embeddings)

Sentence Transformers / Nomic Embed

Optional: FAISS / Chroma (Vector Databases)
